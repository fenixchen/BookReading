{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积运算(Convolution)\n",
    "\n",
    "- 核 == 过滤器\n",
    "- 填充(padding)\n",
    "- 步幅(stride)\n",
    "\n",
    "## 计算输出大小\n",
    "- Input: (H, W)\n",
    "- Filter: (FH, FW)\n",
    "- Padding: P\n",
    "- Stride: S\n",
    "- Output: (OH, HW)\n",
    "\n",
    "`\n",
    "OH = (H + 2 * P - FH) / S +1\n",
    "OW = (W + 2 * P - FW) / S +1\n",
    "`\n",
    "\n",
    "## 三维数据的卷积运算\n",
    "\n",
    "(Channel, Width, Height)\n",
    "\n",
    "(C, FH, FW)\n",
    "\n",
    "多个滤波器: (FN, C, FH, FW)\n",
    "输出数据: (FN, OH, OW)\n",
    "\n",
    "### 批处理\n",
    "(N, C, H, W) * (FN, C, H, W) -> (N, FN, OH, OW) + (FN, 1, 1) -> (N, FN, OH, OW)\n",
    "\n",
    "1. N个C通道的图像(H, W)\n",
    "2. FN个C通道的过滤器\n",
    "3. N个FN个输出(OH, OW)\n",
    "4. N个偏置\n",
    "5. N个FN个输出(OH, OW)\n",
    "\n",
    "## 池化层(Polling)\n",
    "\n",
    "Max池化\n",
    "1. 没有要学习的参数\n",
    "2. 通道数不变\n",
    "\n",
    "# 卷积层和池化层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4维数组\n",
    "import numpy as np\n",
    "\n",
    "# 10个通道为1的28x28的图像\n",
    "x = np.random.rand(10, 1, 28, 28)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于im2col的展开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据\n",
    "    filter_h : 滤波器的高\n",
    "    filter_w : 滤波器的长\n",
    "    stride : 步幅\n",
    "    pad : 填充\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2维数组\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 输入数据的形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 7, 7) 18\n",
      "(9, 75) 84\n",
      "4.666666666666667\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "print(x1.shape, np.sum(x1.shape))\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape, np.sum(col1.shape))\n",
    "print(np.sum(col1.shape) / np.sum(x1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 7, 7) 27\n",
      "(90, 75) 165\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(10, 3, 7, 7)\n",
    "print(x1.shape, np.sum(x1.shape))\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape, np.sum(col1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中间数据（backward时使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 权重和偏置参数的梯度\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('SourceCode')  # 为了导入父目录的文件而进行的设定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "class SimpleConvNet:\n",
    "    \"\"\"简单的ConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 输入大小（MNIST的情况下为784）\n",
    "    hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]）\n",
    "    output_size : 输出大小（MNIST的情况下为10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 指定权重的标准差（e.g. 0.01）\n",
    "        指定'relu'或'he'的情况下设定“He的初始值”\n",
    "        指定'sigmoid'或'xavier'的情况下设定“Xavier的初始值”\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, \n",
    "                 output_size=10, \n",
    "                 weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"求损失函数\n",
    "        参数x是输入数据、t是教师标签\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"求梯度（数值微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 输入数据\n",
    "        t : 教师标签\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        具有各层的梯度的字典变量\n",
    "            grads['W1']、grads['W2']、...是各层的权重\n",
    "            grads['b1']、grads['b2']、...是各层的偏置\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"求梯度（误差反向传播法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 输入数据\n",
    "        t : 教师标签\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        具有各层的梯度的字典变量\n",
    "            grads['W1']、grads['W2']、...是各层的权重\n",
    "            grads['b1']、grads['b2']、...是各层的偏置\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300075529498667\n",
      "=== epoch:1, train acc:0.347, test acc:0.337 ===\n",
      "train loss:2.2976231914909446\n",
      "train loss:2.293250345791921\n",
      "train loss:2.28822831593672\n",
      "train loss:2.2762179142640155\n",
      "train loss:2.265384329176502\n",
      "train loss:2.2453830639417514\n",
      "train loss:2.228250896762913\n",
      "train loss:2.1989768105260565\n",
      "train loss:2.176146614587384\n",
      "train loss:2.1419209883940895\n",
      "train loss:2.074808727553951\n",
      "train loss:2.074019128883002\n",
      "train loss:1.9838108076872403\n",
      "train loss:1.9608504263287967\n",
      "train loss:1.919767377149119\n",
      "train loss:1.8410096651218897\n",
      "train loss:1.7592103904482619\n",
      "train loss:1.660917894109475\n",
      "train loss:1.588977979748135\n",
      "train loss:1.4210663365184073\n",
      "train loss:1.4098360773421894\n",
      "train loss:1.2970590994056093\n",
      "train loss:1.2402570655156981\n",
      "train loss:1.1300891593411497\n",
      "train loss:1.0777503744516785\n",
      "train loss:0.9311281388455438\n",
      "train loss:0.8127188006375071\n",
      "train loss:0.9160272699717268\n",
      "train loss:0.8165279445500923\n",
      "train loss:0.7648235894645693\n",
      "train loss:0.6928373004195139\n",
      "train loss:0.6516420438806239\n",
      "train loss:0.9536035261666571\n",
      "train loss:0.632022024679898\n",
      "train loss:0.6552358636315995\n",
      "train loss:0.6027381843687672\n",
      "train loss:0.6727156914848718\n",
      "train loss:0.748214376132981\n",
      "train loss:0.6792520388293704\n",
      "train loss:0.7870778971393613\n",
      "train loss:0.723821972835989\n",
      "train loss:0.6962970809390159\n",
      "train loss:0.5804110010131315\n",
      "train loss:0.5096703027335355\n",
      "train loss:0.6777979503180086\n",
      "train loss:0.6195019414124963\n",
      "train loss:0.5572519958037949\n",
      "train loss:0.7568853562929201\n",
      "train loss:0.4063327078566934\n",
      "train loss:0.5528000310609044\n",
      "=== epoch:2, train acc:0.824, test acc:0.805 ===\n",
      "train loss:0.6741900302516732\n",
      "train loss:0.4972027731584084\n",
      "train loss:0.4182528127023167\n",
      "train loss:0.6426099716154919\n",
      "train loss:0.4311041902116476\n",
      "train loss:0.5239740154000303\n",
      "train loss:0.5058066191768832\n",
      "train loss:0.4933414060086663\n",
      "train loss:0.5379630260839573\n",
      "train loss:0.30601618621949056\n",
      "train loss:0.5106703681423322\n",
      "train loss:0.3982329235844312\n",
      "train loss:0.43997654885986437\n",
      "train loss:0.48523460733210266\n",
      "train loss:0.4953150508669082\n",
      "train loss:0.3068514325232161\n",
      "train loss:0.6644714067795623\n",
      "train loss:0.4005637674202731\n",
      "train loss:0.43295361671354643\n",
      "train loss:0.46614642868980805\n",
      "train loss:0.3473797584099817\n",
      "train loss:0.47079042034437224\n",
      "train loss:0.48799999167306446\n",
      "train loss:0.2800478212117222\n",
      "train loss:0.475917793699277\n",
      "train loss:0.5755737469589901\n",
      "train loss:0.3631795569229055\n",
      "train loss:0.3335834684995951\n",
      "train loss:0.45865026253584373\n",
      "train loss:0.3538596786546197\n",
      "train loss:0.33745460175446573\n",
      "train loss:0.48220352317166776\n",
      "train loss:0.43091268756559237\n",
      "train loss:0.39495438280631456\n",
      "train loss:0.4663915003444283\n",
      "train loss:0.4080597562542817\n",
      "train loss:0.490502638813177\n",
      "train loss:0.40239012361078275\n",
      "train loss:0.3771121363400485\n",
      "train loss:0.3194336805750247\n",
      "train loss:0.42292899358435404\n",
      "train loss:0.3285866135687374\n",
      "train loss:0.4287375219253677\n",
      "train loss:0.3722882570474544\n",
      "train loss:0.333001997772161\n",
      "train loss:0.34271513343355087\n",
      "train loss:0.3731431595001078\n",
      "train loss:0.49480369472652036\n",
      "train loss:0.5077078861060064\n",
      "train loss:0.3510338917102766\n",
      "=== epoch:3, train acc:0.872, test acc:0.878 ===\n",
      "train loss:0.29035961735926547\n",
      "train loss:0.3048752993846973\n",
      "train loss:0.3404089217693595\n",
      "train loss:0.4051088558580878\n",
      "train loss:0.2922730720923157\n",
      "train loss:0.36707206171173745\n",
      "train loss:0.29658994094033597\n",
      "train loss:0.25063923877186883\n",
      "train loss:0.29257001574004915\n",
      "train loss:0.5025211445614144\n",
      "train loss:0.5556170948220454\n",
      "train loss:0.3944650912777496\n",
      "train loss:0.45661864621763465\n",
      "train loss:0.5344598991641247\n",
      "train loss:0.3862239361539601\n",
      "train loss:0.23700912300154925\n",
      "train loss:0.2171740482059192\n",
      "train loss:0.20827435624439963\n",
      "train loss:0.41540705444483955\n",
      "train loss:0.43708191279795594\n",
      "train loss:0.42029855769413854\n",
      "train loss:0.24854395228341275\n",
      "train loss:0.2264959986291843\n",
      "train loss:0.1368884801067229\n",
      "train loss:0.29299818172989806\n",
      "train loss:0.29436557942433644\n",
      "train loss:0.261716467425099\n",
      "train loss:0.32221139837197554\n",
      "train loss:0.27989922413399343\n",
      "train loss:0.16116477414794386\n",
      "train loss:0.37682816031294913\n",
      "train loss:0.36529840601942487\n",
      "train loss:0.2600183491532552\n",
      "train loss:0.33195148732043167\n",
      "train loss:0.21240591189304323\n",
      "train loss:0.33079682173593405\n",
      "train loss:0.34155302435446927\n",
      "train loss:0.18891352192268093\n",
      "train loss:0.2227182253083254\n",
      "train loss:0.2086827971720027\n",
      "train loss:0.3427724965125612\n",
      "train loss:0.2841843270600111\n",
      "train loss:0.28949948960239885\n",
      "train loss:0.2564676149552436\n",
      "train loss:0.25676478029210265\n",
      "train loss:0.15469714420148958\n",
      "train loss:0.3291948418819232\n",
      "train loss:0.18957084274015504\n",
      "train loss:0.23720827251935894\n",
      "train loss:0.19345651554639726\n",
      "=== epoch:4, train acc:0.895, test acc:0.884 ===\n",
      "train loss:0.277341937317705\n",
      "train loss:0.17941988824602947\n",
      "train loss:0.4192974128681336\n",
      "train loss:0.38038492677502556\n",
      "train loss:0.27339300112130066\n",
      "train loss:0.2959319063775577\n",
      "train loss:0.26046511323169\n",
      "train loss:0.40008689798321945\n",
      "train loss:0.48184815821632015\n",
      "train loss:0.28098330875050337\n",
      "train loss:0.20782491297966246\n",
      "train loss:0.31718471605855386\n",
      "train loss:0.2202998928722333\n",
      "train loss:0.15580587443046545\n",
      "train loss:0.29418973097487705\n",
      "train loss:0.2678235608265136\n",
      "train loss:0.3386073412808032\n",
      "train loss:0.1712608637507734\n",
      "train loss:0.18237008248140682\n",
      "train loss:0.3073508530701565\n",
      "train loss:0.32337885149162543\n",
      "train loss:0.2039126531200916\n",
      "train loss:0.28129972070584086\n",
      "train loss:0.37432578696587465\n",
      "train loss:0.22081650325899968\n",
      "train loss:0.2688017400787326\n",
      "train loss:0.12064135218565401\n",
      "train loss:0.21439146510777976\n",
      "train loss:0.42017787913834886\n",
      "train loss:0.34340385896020675\n",
      "train loss:0.2175063063128225\n",
      "train loss:0.2943359937972641\n",
      "train loss:0.2890539686169086\n",
      "train loss:0.42058601712173804\n",
      "train loss:0.18855769208236897\n",
      "train loss:0.3370180956373398\n",
      "train loss:0.17892610518314433\n",
      "train loss:0.27682220265176666\n",
      "train loss:0.19690667378251114\n",
      "train loss:0.2721734511208013\n",
      "train loss:0.2100022991932075\n",
      "train loss:0.18556487936052513\n",
      "train loss:0.23008530187957882\n",
      "train loss:0.1891308830300844\n",
      "train loss:0.16692950213697813\n",
      "train loss:0.2531555728553624\n",
      "train loss:0.15788774053749083\n",
      "train loss:0.2551449772739417\n",
      "train loss:0.26407167660132613\n",
      "train loss:0.30089051928482163\n",
      "=== epoch:5, train acc:0.906, test acc:0.897 ===\n",
      "train loss:0.16713481579927478\n",
      "train loss:0.24921426086274717\n",
      "train loss:0.31538020830069735\n",
      "train loss:0.1284847377506341\n",
      "train loss:0.19457550243442895\n",
      "train loss:0.3078245712437635\n",
      "train loss:0.22276041896945034\n",
      "train loss:0.3612913308215808\n",
      "train loss:0.26849022433594444\n",
      "train loss:0.22961761622041166\n",
      "train loss:0.2609824827610913\n",
      "train loss:0.3364690435684865\n",
      "train loss:0.1471338994330034\n",
      "train loss:0.1241137033389716\n",
      "train loss:0.14994906613864528\n",
      "train loss:0.13873912049183346\n",
      "train loss:0.2222978167644674\n",
      "train loss:0.17427222956429833\n",
      "train loss:0.24826003233398963\n",
      "train loss:0.3010910247094331\n",
      "train loss:0.13280685673560147\n",
      "train loss:0.23309471675762833\n",
      "train loss:0.38769587439472614\n",
      "train loss:0.19368038255373013\n",
      "train loss:0.14428735213717975\n",
      "train loss:0.331744792869447\n",
      "train loss:0.19420704760078628\n",
      "train loss:0.2805807758102356\n",
      "train loss:0.13649873504248297\n",
      "train loss:0.2050933646903149\n",
      "train loss:0.08155691085091338\n",
      "train loss:0.1891492542962187\n",
      "train loss:0.328932264713019\n",
      "train loss:0.16416041389600963\n",
      "train loss:0.3187047264165031\n",
      "train loss:0.10278003296261044\n",
      "train loss:0.22003299449753203\n",
      "train loss:0.19353153799878847\n",
      "train loss:0.2465983222692049\n",
      "train loss:0.2114098845536935\n",
      "train loss:0.20976000544917153\n",
      "train loss:0.31308280787970216\n",
      "train loss:0.24363846490676314\n",
      "train loss:0.2084757450151647\n",
      "train loss:0.18999460182707306\n",
      "train loss:0.25165618536967843\n",
      "train loss:0.1907606367830978\n",
      "train loss:0.23136097904295178\n",
      "train loss:0.22637519276568163\n",
      "train loss:0.15946554756265058\n",
      "=== epoch:6, train acc:0.917, test acc:0.903 ===\n",
      "train loss:0.1629811458411062\n",
      "train loss:0.25858319790763973\n",
      "train loss:0.13332120365422448\n",
      "train loss:0.27283272729261265\n",
      "train loss:0.19006412857059898\n",
      "train loss:0.285054448315161\n",
      "train loss:0.37959562125003116\n",
      "train loss:0.2668421909125657\n",
      "train loss:0.19816895774981447\n",
      "train loss:0.17457638481924756\n",
      "train loss:0.19270962835017888\n",
      "train loss:0.1556709852335579\n",
      "train loss:0.14481500731944752\n",
      "train loss:0.21390898181267112\n",
      "train loss:0.24837708728197747\n",
      "train loss:0.0822615885735054\n",
      "train loss:0.12393708067923948\n",
      "train loss:0.09777406779831656\n",
      "train loss:0.2891749297777253\n",
      "train loss:0.20987093387633243\n",
      "train loss:0.2759644418140844\n",
      "train loss:0.2647588776134275\n",
      "train loss:0.12271842167654662\n",
      "train loss:0.23504398346066438\n",
      "train loss:0.26051897264198565\n",
      "train loss:0.23079517055926949\n",
      "train loss:0.1270920399745184\n",
      "train loss:0.1487422296069797\n",
      "train loss:0.25649566593318\n",
      "train loss:0.18693676067274625\n",
      "train loss:0.30056626035144157\n",
      "train loss:0.19330224357913486\n",
      "train loss:0.36712174397888175\n",
      "train loss:0.24345736629328235\n",
      "train loss:0.23924127475224563\n",
      "train loss:0.24166603341805215\n",
      "train loss:0.12735641242751528\n",
      "train loss:0.12800758695348946\n",
      "train loss:0.24590666349817678\n",
      "train loss:0.1689088992870106\n",
      "train loss:0.167408466661903\n",
      "train loss:0.20707536329212017\n",
      "train loss:0.23420644350318093\n",
      "train loss:0.14439743808274366\n",
      "train loss:0.12427689316897787\n",
      "train loss:0.18873194896475173\n",
      "train loss:0.28257094639714336\n",
      "train loss:0.07741111286934475\n",
      "train loss:0.2790770374325089\n",
      "train loss:0.1378768449221833\n",
      "=== epoch:7, train acc:0.935, test acc:0.91 ===\n",
      "train loss:0.1158576162562578\n",
      "train loss:0.09161838760315907\n",
      "train loss:0.343563929373655\n",
      "train loss:0.14464650454509625\n",
      "train loss:0.19915787857987613\n",
      "train loss:0.16240121568703791\n",
      "train loss:0.14836127418608336\n",
      "train loss:0.07348212469640304\n",
      "train loss:0.12123274310193613\n",
      "train loss:0.20555309949096653\n",
      "train loss:0.15314641099149248\n",
      "train loss:0.133369293080263\n",
      "train loss:0.08968907086009223\n",
      "train loss:0.16250539832014493\n",
      "train loss:0.12932562920204094\n",
      "train loss:0.150060132930419\n",
      "train loss:0.19059671980675696\n",
      "train loss:0.19232685957314274\n",
      "train loss:0.1825487477587028\n",
      "train loss:0.1298345041735782\n",
      "train loss:0.12213166631286695\n",
      "train loss:0.22496210709417297\n",
      "train loss:0.309013540272978\n",
      "train loss:0.12175191234775572\n",
      "train loss:0.24228662981836627\n",
      "train loss:0.16352698129376886\n",
      "train loss:0.2558541243929383\n",
      "train loss:0.2839821548586601\n",
      "train loss:0.1452172520744309\n",
      "train loss:0.11798071220907351\n",
      "train loss:0.2015999571871424\n",
      "train loss:0.14563978702673364\n",
      "train loss:0.08273092269706646\n",
      "train loss:0.10668293257529644\n",
      "train loss:0.08139011576093226\n",
      "train loss:0.09117143499463433\n",
      "train loss:0.1736715935962944\n",
      "train loss:0.16826612393575668\n",
      "train loss:0.10215690937949935\n",
      "train loss:0.06678445127157623\n",
      "train loss:0.17976577523151324\n",
      "train loss:0.223386703034468\n",
      "train loss:0.25195316804430573\n",
      "train loss:0.05797529882362142\n",
      "train loss:0.15541562283828006\n",
      "train loss:0.16502964148919788\n",
      "train loss:0.13401886411959169\n",
      "train loss:0.20897883281456303\n",
      "train loss:0.2308562107640565\n",
      "train loss:0.19600670156443573\n",
      "=== epoch:8, train acc:0.943, test acc:0.923 ===\n",
      "train loss:0.1784429426441317\n",
      "train loss:0.17382553697068523\n",
      "train loss:0.21634416680175783\n",
      "train loss:0.14027737335943835\n",
      "train loss:0.1414472863550326\n",
      "train loss:0.19824636971498466\n",
      "train loss:0.13662875614674994\n",
      "train loss:0.14006334434883927\n",
      "train loss:0.18992972380874346\n",
      "train loss:0.13309722576350366\n",
      "train loss:0.09433508617580752\n",
      "train loss:0.09425359889556605\n",
      "train loss:0.2622901051822992\n",
      "train loss:0.12509987020074512\n",
      "train loss:0.3508782309857774\n",
      "train loss:0.1708976642530238\n",
      "train loss:0.12243419848606697\n",
      "train loss:0.16011210648416183\n",
      "train loss:0.10189552940059658\n",
      "train loss:0.1928148170571746\n",
      "train loss:0.18399760358734046\n",
      "train loss:0.2505503105599311\n",
      "train loss:0.1399752406046653\n",
      "train loss:0.1680165090781329\n",
      "train loss:0.0945921800147121\n",
      "train loss:0.14176248244420134\n",
      "train loss:0.08606968386052627\n",
      "train loss:0.12612902738757284\n",
      "train loss:0.1303634443113784\n",
      "train loss:0.13502846964199708\n",
      "train loss:0.09872831659444646\n",
      "train loss:0.17137027041988645\n",
      "train loss:0.12737334769892816\n",
      "train loss:0.07029906793786905\n",
      "train loss:0.16261294207256669\n",
      "train loss:0.24099238080806917\n",
      "train loss:0.2545743123454236\n",
      "train loss:0.14390802465897892\n",
      "train loss:0.155270206069628\n",
      "train loss:0.1013448499827013\n",
      "train loss:0.17590424544007607\n",
      "train loss:0.1278681021122576\n",
      "train loss:0.14533973139680842\n",
      "train loss:0.17285711192872522\n",
      "train loss:0.1129701259817123\n",
      "train loss:0.14379644661853608\n",
      "train loss:0.18836673713141125\n",
      "train loss:0.2114840380520283\n",
      "train loss:0.090072304925014\n",
      "train loss:0.08128275614606048\n",
      "=== epoch:9, train acc:0.95, test acc:0.93 ===\n",
      "train loss:0.18468713499134087\n",
      "train loss:0.1677739381081178\n",
      "train loss:0.09556743715127723\n",
      "train loss:0.11433693478655604\n",
      "train loss:0.11043516126205732\n",
      "train loss:0.10890416359553352\n",
      "train loss:0.13066128216802317\n",
      "train loss:0.14501643035140158\n",
      "train loss:0.11415897039167426\n",
      "train loss:0.1269010599866357\n",
      "train loss:0.128803729095659\n",
      "train loss:0.13036149407435807\n",
      "train loss:0.08481950300676232\n",
      "train loss:0.15244798855252062\n",
      "train loss:0.12985382984401742\n",
      "train loss:0.1677780453522367\n",
      "train loss:0.16141916394217115\n",
      "train loss:0.10770974572798214\n",
      "train loss:0.12022968627154311\n",
      "train loss:0.15355593047007915\n",
      "train loss:0.06181585131170423\n",
      "train loss:0.17398121401632236\n",
      "train loss:0.06086631227882774\n",
      "train loss:0.12887112538779374\n",
      "train loss:0.14915361435485366\n",
      "train loss:0.14378261419356422\n",
      "train loss:0.13588362954928562\n",
      "train loss:0.0955374164543001\n",
      "train loss:0.08484312401263815\n",
      "train loss:0.03136368360601729\n",
      "train loss:0.08883864415675388\n",
      "train loss:0.06291183865806613\n",
      "train loss:0.1473008225155902\n",
      "train loss:0.1541452793214065\n",
      "train loss:0.15125997087409324\n",
      "train loss:0.08817746404587629\n",
      "train loss:0.08956333873058425\n",
      "train loss:0.1711934033555044\n",
      "train loss:0.07549824554233503\n",
      "train loss:0.19258979232929824\n",
      "train loss:0.125580815234434\n",
      "train loss:0.13048680398457266\n",
      "train loss:0.09499800241004425\n",
      "train loss:0.07921834473744686\n",
      "train loss:0.2381387179523097\n",
      "train loss:0.11332370830202487\n",
      "train loss:0.11195361350820017\n",
      "train loss:0.11795656085314396\n",
      "train loss:0.10450277127841856\n",
      "train loss:0.1998214209077109\n",
      "=== epoch:10, train acc:0.958, test acc:0.945 ===\n",
      "train loss:0.23123191249112104\n",
      "train loss:0.1585612763506582\n",
      "train loss:0.0984791409012728\n",
      "train loss:0.12619112126547438\n",
      "train loss:0.08338450378923148\n",
      "train loss:0.06383263406930795\n",
      "train loss:0.12669063154347476\n",
      "train loss:0.20539479996102583\n",
      "train loss:0.10592702383762527\n",
      "train loss:0.16302340131311382\n",
      "train loss:0.127812539231596\n",
      "train loss:0.09729568696848764\n",
      "train loss:0.1722288688823092\n",
      "train loss:0.09328036611354358\n",
      "train loss:0.06513922757645911\n",
      "train loss:0.1438263175590002\n",
      "train loss:0.06429787527758238\n",
      "train loss:0.12351407709744462\n",
      "train loss:0.1344542879169841\n",
      "train loss:0.078354586050919\n",
      "train loss:0.11816888211730166\n",
      "train loss:0.1211496707345058\n",
      "train loss:0.0819277009721346\n",
      "train loss:0.061901234781548437\n",
      "train loss:0.16325278265865242\n",
      "train loss:0.07982746205310183\n",
      "train loss:0.12827021020205312\n",
      "train loss:0.14789496540374858\n",
      "train loss:0.0879821127339691\n",
      "train loss:0.1436403205197308\n",
      "train loss:0.07854675422074223\n",
      "train loss:0.04836760277583847\n",
      "train loss:0.031044582049302553\n",
      "train loss:0.12891829206195893\n",
      "train loss:0.09970383475496858\n",
      "train loss:0.05598565766530461\n",
      "train loss:0.08270230577017014\n",
      "train loss:0.11763409210497029\n",
      "train loss:0.1407233573899461\n",
      "train loss:0.19005456751757155\n",
      "train loss:0.07960094796194742\n",
      "train loss:0.09252542405789362\n",
      "train loss:0.06410884161808653\n",
      "train loss:0.15440939210579832\n",
      "train loss:0.04317443870757664\n",
      "train loss:0.11083635864526836\n",
      "train loss:0.04618925984601738\n",
      "train loss:0.16732570493963156\n",
      "train loss:0.13099006364434237\n",
      "train loss:0.10892200117161416\n",
      "=== epoch:11, train acc:0.961, test acc:0.937 ===\n",
      "train loss:0.08086076991574048\n",
      "train loss:0.10237428149306962\n",
      "train loss:0.06819216320484432\n",
      "train loss:0.0706999888856391\n",
      "train loss:0.03549865867489206\n",
      "train loss:0.15434046959497372\n",
      "train loss:0.03141280296959975\n",
      "train loss:0.14888764051595563\n",
      "train loss:0.08059943880779485\n",
      "train loss:0.06063923591323982\n",
      "train loss:0.06637073282256521\n",
      "train loss:0.15941239773173543\n",
      "train loss:0.08550877748381115\n",
      "train loss:0.0866863221120306\n",
      "train loss:0.08800739559122367\n",
      "train loss:0.11432100785523387\n",
      "train loss:0.18966530652651564\n",
      "train loss:0.08903836386501107\n",
      "train loss:0.0678610663955063\n",
      "train loss:0.17035751992962758\n",
      "train loss:0.0854240000815146\n",
      "train loss:0.08984726116593654\n",
      "train loss:0.12086973776478885\n",
      "train loss:0.08569638758209683\n",
      "train loss:0.07733651039429795\n",
      "train loss:0.07159592100319624\n",
      "train loss:0.05835263465826378\n",
      "train loss:0.057516058829745066\n",
      "train loss:0.20920953626472202\n",
      "train loss:0.14957890183721165\n",
      "train loss:0.12841299624443464\n",
      "train loss:0.15018369818705762\n",
      "train loss:0.13025352721055192\n",
      "train loss:0.03799919649330896\n",
      "train loss:0.08021538869370129\n",
      "train loss:0.12684499493399884\n",
      "train loss:0.12409276296666004\n",
      "train loss:0.11288408269294078\n",
      "train loss:0.04654161443690304\n",
      "train loss:0.1917379399662154\n",
      "train loss:0.1328154476879816\n",
      "train loss:0.08700732958412521\n",
      "train loss:0.08877381619050746\n",
      "train loss:0.06256512565429317\n",
      "train loss:0.04860409147503197\n",
      "train loss:0.06976005568203396\n",
      "train loss:0.08380198778023608\n",
      "train loss:0.043242182030377815\n",
      "train loss:0.08449773449630273\n",
      "train loss:0.08249501555378037\n",
      "=== epoch:12, train acc:0.962, test acc:0.941 ===\n",
      "train loss:0.13892250852057797\n",
      "train loss:0.08970354795957926\n",
      "train loss:0.06289483491550231\n",
      "train loss:0.06600826922363594\n",
      "train loss:0.08579504379955895\n",
      "train loss:0.07075824300637575\n",
      "train loss:0.06791729149153986\n",
      "train loss:0.05249942481039497\n",
      "train loss:0.04340493917799569\n",
      "train loss:0.04514657255884473\n",
      "train loss:0.07529913564340383\n",
      "train loss:0.04929844564949091\n",
      "train loss:0.024047801537705648\n",
      "train loss:0.07361567093057848\n",
      "train loss:0.15511540912059632\n",
      "train loss:0.09778691491159064\n",
      "train loss:0.13176701358998352\n",
      "train loss:0.0926462631363539\n",
      "train loss:0.06538808894467707\n",
      "train loss:0.0554963201523774\n",
      "train loss:0.06526927914139483\n",
      "train loss:0.06401517745472923\n",
      "train loss:0.11988710827188344\n",
      "train loss:0.025658609906048176\n",
      "train loss:0.060691516136638216\n",
      "train loss:0.1546572572885486\n",
      "train loss:0.04841851439295585\n",
      "train loss:0.0975551307970199\n",
      "train loss:0.1038522720817355\n",
      "train loss:0.06543846524355688\n",
      "train loss:0.08449281909247926\n",
      "train loss:0.05967020325277476\n",
      "train loss:0.035588645910765375\n",
      "train loss:0.06795636180678855\n",
      "train loss:0.04694707403188532\n",
      "train loss:0.06711996133517178\n",
      "train loss:0.06076944188448877\n",
      "train loss:0.11699151108469706\n",
      "train loss:0.09116132318408633\n",
      "train loss:0.05519149344862516\n",
      "train loss:0.12150620611192879\n",
      "train loss:0.1422721668265163\n",
      "train loss:0.0382973890819179\n",
      "train loss:0.11068909645345182\n",
      "train loss:0.05761833083852677\n",
      "train loss:0.06469955345648452\n",
      "train loss:0.0761559152595881\n",
      "train loss:0.08320567141863647\n",
      "train loss:0.0734167563606356\n",
      "train loss:0.08875386469100707\n",
      "=== epoch:13, train acc:0.969, test acc:0.944 ===\n",
      "train loss:0.04365051255792182\n",
      "train loss:0.09516864918655904\n",
      "train loss:0.08830185893248232\n",
      "train loss:0.07438993285660966\n",
      "train loss:0.08178857514008725\n",
      "train loss:0.0828528704003884\n",
      "train loss:0.09765371458540134\n",
      "train loss:0.09003965538823679\n",
      "train loss:0.0529698557026943\n",
      "train loss:0.08166118953701808\n",
      "train loss:0.08896666889709684\n",
      "train loss:0.04676195766039143\n",
      "train loss:0.11680145209990762\n",
      "train loss:0.05067049906276023\n",
      "train loss:0.08144379872262278\n",
      "train loss:0.08634578616325549\n",
      "train loss:0.03787825140251875\n",
      "train loss:0.11911461176526637\n",
      "train loss:0.07617671470205684\n",
      "train loss:0.17293519513090033\n",
      "train loss:0.11354736243407933\n",
      "train loss:0.07642760704414911\n",
      "train loss:0.06922307336573345\n",
      "train loss:0.05325224071557139\n",
      "train loss:0.04844802478954639\n",
      "train loss:0.1342772168477201\n",
      "train loss:0.059414940141537524\n",
      "train loss:0.0428865495729155\n",
      "train loss:0.110834701170388\n",
      "train loss:0.15048939432881756\n",
      "train loss:0.11399032806758992\n",
      "train loss:0.037218623413295605\n",
      "train loss:0.04413941550900057\n",
      "train loss:0.06540428954572382\n",
      "train loss:0.14515948705054957\n",
      "train loss:0.17534265794082893\n",
      "train loss:0.07100801962802888\n",
      "train loss:0.05136498297171178\n",
      "train loss:0.03493037085507407\n",
      "train loss:0.05993645411007347\n",
      "train loss:0.09694135083944326\n",
      "train loss:0.06804198018961898\n",
      "train loss:0.07922176592545369\n",
      "train loss:0.0804215215043819\n",
      "train loss:0.07688897647834392\n",
      "train loss:0.1310254679700138\n",
      "train loss:0.11365307674324705\n",
      "train loss:0.08038685305648982\n",
      "train loss:0.05985879571256759\n",
      "train loss:0.07624214767994583\n",
      "=== epoch:14, train acc:0.964, test acc:0.943 ===\n",
      "train loss:0.07563697368795691\n",
      "train loss:0.09715532421734163\n",
      "train loss:0.04191725529341386\n",
      "train loss:0.11601753709838507\n",
      "train loss:0.07911945190053503\n",
      "train loss:0.03820693725406252\n",
      "train loss:0.08218652893274245\n",
      "train loss:0.05449691109740429\n",
      "train loss:0.09586713232001605\n",
      "train loss:0.0470463422710578\n",
      "train loss:0.05018598907197207\n",
      "train loss:0.02199183794279811\n",
      "train loss:0.10802358701136297\n",
      "train loss:0.02009428917982887\n",
      "train loss:0.03740889357277121\n",
      "train loss:0.06790969055232852\n",
      "train loss:0.1923009022677864\n",
      "train loss:0.0654961532848006\n",
      "train loss:0.06911037165857786\n",
      "train loss:0.049198942129183416\n",
      "train loss:0.07558746685422692\n",
      "train loss:0.07160970576637295\n",
      "train loss:0.12150190065833064\n",
      "train loss:0.058883669670895325\n",
      "train loss:0.05244938635858937\n",
      "train loss:0.057864400369844304\n",
      "train loss:0.04953155746397064\n",
      "train loss:0.09058079418386096\n",
      "train loss:0.026192347135423982\n",
      "train loss:0.05985487936498449\n",
      "train loss:0.036102634345832094\n",
      "train loss:0.064891504332104\n",
      "train loss:0.034474857466174555\n",
      "train loss:0.023723732143393537\n",
      "train loss:0.08245569351782855\n",
      "train loss:0.051327943447477174\n",
      "train loss:0.13476118381772534\n",
      "train loss:0.03793084014442025\n",
      "train loss:0.057971948778768684\n",
      "train loss:0.030025186271880354\n",
      "train loss:0.03790728246956301\n",
      "train loss:0.04767414075368112\n",
      "train loss:0.02429705306813723\n",
      "train loss:0.06348049125334784\n",
      "train loss:0.04283601492295227\n",
      "train loss:0.10453487041909498\n",
      "train loss:0.09095376721825221\n",
      "train loss:0.05227209441694768\n",
      "train loss:0.07123326962494626\n",
      "train loss:0.08805313725531608\n",
      "=== epoch:15, train acc:0.975, test acc:0.932 ===\n",
      "train loss:0.12196674919029939\n",
      "train loss:0.04698099671986651\n",
      "train loss:0.07249352330456042\n",
      "train loss:0.08227805842159942\n",
      "train loss:0.05333445187807941\n",
      "train loss:0.07460114761277796\n",
      "train loss:0.08029890474056972\n",
      "train loss:0.03883856297894339\n",
      "train loss:0.03691207664144058\n",
      "train loss:0.03867985578384026\n",
      "train loss:0.06674771138109492\n",
      "train loss:0.04681482022629422\n",
      "train loss:0.06419789847452803\n",
      "train loss:0.06732810729261328\n",
      "train loss:0.09582610822304558\n",
      "train loss:0.044279585373769484\n",
      "train loss:0.039969107414608754\n",
      "train loss:0.11837879035560515\n",
      "train loss:0.10455799559733134\n",
      "train loss:0.03118757084953598\n",
      "train loss:0.0490795664601662\n",
      "train loss:0.04613957128615348\n",
      "train loss:0.04753645369739795\n",
      "train loss:0.051343034313815955\n",
      "train loss:0.051445410327502605\n",
      "train loss:0.03771250031106541\n",
      "train loss:0.06987778053814032\n",
      "train loss:0.05865804365149677\n",
      "train loss:0.03465620436343078\n",
      "train loss:0.050470585970180656\n",
      "train loss:0.04138103096282223\n",
      "train loss:0.06515456385105656\n",
      "train loss:0.06922731175687022\n",
      "train loss:0.0659194434871719\n",
      "train loss:0.05527353644338609\n",
      "train loss:0.02379529111125416\n",
      "train loss:0.045544981845956546\n",
      "train loss:0.05141060679443929\n",
      "train loss:0.021275028773791897\n",
      "train loss:0.04723803575510628\n",
      "train loss:0.04340053117336112\n",
      "train loss:0.04016655044885752\n",
      "train loss:0.03898001462416246\n",
      "train loss:0.04319759865494925\n",
      "train loss:0.04847242600224042\n",
      "train loss:0.11712658821984412\n",
      "train loss:0.017721860319184213\n",
      "train loss:0.033638941521502416\n",
      "train loss:0.03385361468662775\n",
      "train loss:0.10949288842746423\n",
      "=== epoch:16, train acc:0.975, test acc:0.949 ===\n",
      "train loss:0.05012550126649348\n",
      "train loss:0.061448445770961736\n",
      "train loss:0.06097591000394519\n",
      "train loss:0.037897804194982655\n",
      "train loss:0.07324891635686717\n",
      "train loss:0.08400731729845948\n",
      "train loss:0.029081351649597326\n",
      "train loss:0.03647137171942352\n",
      "train loss:0.04036862126521124\n",
      "train loss:0.13568341150085067\n",
      "train loss:0.06822885339070081\n",
      "train loss:0.09040126112337304\n",
      "train loss:0.11926758449456866\n",
      "train loss:0.02228231670582139\n",
      "train loss:0.02870428555624335\n",
      "train loss:0.059335221496538235\n",
      "train loss:0.024117505164322067\n",
      "train loss:0.04045064514745077\n",
      "train loss:0.08561309403491746\n",
      "train loss:0.059653322061946186\n",
      "train loss:0.04135572996346762\n",
      "train loss:0.08579622476972093\n",
      "train loss:0.020799927574096565\n",
      "train loss:0.07609332452587789\n",
      "train loss:0.025143633176456598\n",
      "train loss:0.06644915879328975\n",
      "train loss:0.04662865491336406\n",
      "train loss:0.022110299384921858\n",
      "train loss:0.019088819606851538\n",
      "train loss:0.05812526819963806\n",
      "train loss:0.05575201288072175\n",
      "train loss:0.031291752667850045\n",
      "train loss:0.03040758875445889\n",
      "train loss:0.018426838066151866\n",
      "train loss:0.026472135517931133\n",
      "train loss:0.027915463032057462\n",
      "train loss:0.037120759559785166\n",
      "train loss:0.05446748656370297\n",
      "train loss:0.016416683856698432\n",
      "train loss:0.018640772742111476\n",
      "train loss:0.06555891036809836\n",
      "train loss:0.03605226118503969\n",
      "train loss:0.03689420842461563\n",
      "train loss:0.043418436827349235\n",
      "train loss:0.0788481092024744\n",
      "train loss:0.05003617732552989\n",
      "train loss:0.0658316006317802\n",
      "train loss:0.03898153678792237\n",
      "train loss:0.019087686225631027\n",
      "train loss:0.03706179479719449\n",
      "=== epoch:17, train acc:0.983, test acc:0.946 ===\n",
      "train loss:0.02502603678503317\n",
      "train loss:0.03450243963303849\n",
      "train loss:0.07539351783422642\n",
      "train loss:0.06615343666669542\n",
      "train loss:0.05912164502445349\n",
      "train loss:0.029600394896960563\n",
      "train loss:0.04010739393994109\n",
      "train loss:0.06498646819116068\n",
      "train loss:0.023114265713794775\n",
      "train loss:0.012818405601229573\n",
      "train loss:0.04155989390244816\n",
      "train loss:0.06604726010969844\n",
      "train loss:0.05888193014494221\n",
      "train loss:0.05095220991614708\n",
      "train loss:0.04536615095089059\n",
      "train loss:0.07416774537912653\n",
      "train loss:0.05740818174796905\n",
      "train loss:0.05570988467444957\n",
      "train loss:0.039491524564261214\n",
      "train loss:0.0558627268728221\n",
      "train loss:0.024049921761151637\n",
      "train loss:0.04899340539443704\n",
      "train loss:0.07893484136694336\n",
      "train loss:0.0566143491493498\n",
      "train loss:0.059928298282127763\n",
      "train loss:0.02347794512095997\n",
      "train loss:0.1604723243397939\n",
      "train loss:0.02906824000920356\n",
      "train loss:0.04174781588872655\n",
      "train loss:0.06268855116394433\n",
      "train loss:0.06420250148800777\n",
      "train loss:0.054851342617149614\n",
      "train loss:0.059806796075458774\n",
      "train loss:0.037316367942092296\n",
      "train loss:0.06445117681966689\n",
      "train loss:0.017367250812144477\n",
      "train loss:0.057829919911957114\n",
      "train loss:0.03165276081939431\n",
      "train loss:0.04175357790580518\n",
      "train loss:0.05977605962172103\n",
      "train loss:0.025711505400797665\n",
      "train loss:0.11728456103320668\n",
      "train loss:0.026681701775746802\n",
      "train loss:0.02557857118765434\n",
      "train loss:0.05153109034787092\n",
      "train loss:0.04745447612286049\n",
      "train loss:0.0404152246997145\n",
      "train loss:0.03944443996070693\n",
      "train loss:0.04243334847762125\n",
      "train loss:0.04051845637215432\n",
      "=== epoch:18, train acc:0.977, test acc:0.946 ===\n",
      "train loss:0.03068107514792976\n",
      "train loss:0.050984720939556964\n",
      "train loss:0.03616550824874722\n",
      "train loss:0.015777688673492763\n",
      "train loss:0.05814443071938489\n",
      "train loss:0.04344553294802458\n",
      "train loss:0.01984299010668172\n",
      "train loss:0.03230863801581433\n",
      "train loss:0.026571497081663432\n",
      "train loss:0.018314265685219423\n",
      "train loss:0.04939736527054153\n",
      "train loss:0.06251808914530042\n",
      "train loss:0.021048302969646073\n",
      "train loss:0.03643453059418688\n",
      "train loss:0.04807442164219489\n",
      "train loss:0.01843262698451886\n",
      "train loss:0.09116821625613075\n",
      "train loss:0.01353491546418789\n",
      "train loss:0.0170435730106022\n",
      "train loss:0.03313139967128917\n",
      "train loss:0.032049370312040956\n",
      "train loss:0.021565416740438402\n",
      "train loss:0.025414687551321105\n",
      "train loss:0.07906105783164463\n",
      "train loss:0.030734360732644878\n",
      "train loss:0.02893161962009995\n",
      "train loss:0.047007016447622396\n",
      "train loss:0.027510537953301897\n",
      "train loss:0.019608735795251624\n",
      "train loss:0.029763366966660196\n",
      "train loss:0.08322226035213738\n",
      "train loss:0.034060971669371234\n",
      "train loss:0.04247360144335872\n",
      "train loss:0.008444431888447129\n",
      "train loss:0.05160194854809607\n",
      "train loss:0.07925148250276423\n",
      "train loss:0.021098843962890385\n",
      "train loss:0.054643818671097966\n",
      "train loss:0.027692380726993714\n",
      "train loss:0.029256923883547472\n",
      "train loss:0.019770192763130613\n",
      "train loss:0.05838930521733943\n",
      "train loss:0.021152264805438182\n",
      "train loss:0.04707913726325583\n",
      "train loss:0.014484157553145756\n",
      "train loss:0.027784528813460906\n",
      "train loss:0.02672106848189137\n",
      "train loss:0.01595915685874982\n",
      "train loss:0.07394812452910446\n",
      "train loss:0.012212396794461908\n",
      "=== epoch:19, train acc:0.985, test acc:0.96 ===\n",
      "train loss:0.02561011209836907\n",
      "train loss:0.017213303294271433\n",
      "train loss:0.056800670917724894\n",
      "train loss:0.014303955557205048\n",
      "train loss:0.0745537191637469\n",
      "train loss:0.04224052592751927\n",
      "train loss:0.027713784964266575\n",
      "train loss:0.02710574717738706\n",
      "train loss:0.034071915689822575\n",
      "train loss:0.027338516464964272\n",
      "train loss:0.034109918357376035\n",
      "train loss:0.017999147413811974\n",
      "train loss:0.014015544539052158\n",
      "train loss:0.012544361895358132\n",
      "train loss:0.027277382241407445\n",
      "train loss:0.044005586877640425\n",
      "train loss:0.019007409729928017\n",
      "train loss:0.014192640753901309\n",
      "train loss:0.03093947917041257\n",
      "train loss:0.03684750797607546\n",
      "train loss:0.023911014967637638\n",
      "train loss:0.018606177549626207\n",
      "train loss:0.07348741076326767\n",
      "train loss:0.019801268002779312\n",
      "train loss:0.05539569017027305\n",
      "train loss:0.04009557562689636\n",
      "train loss:0.011408241614510586\n",
      "train loss:0.051170994506740716\n",
      "train loss:0.017794398071227048\n",
      "train loss:0.019181433752289537\n",
      "train loss:0.012338229334055522\n",
      "train loss:0.010683643375672876\n",
      "train loss:0.02518183145631067\n",
      "train loss:0.02647178244536018\n",
      "train loss:0.020194700407560905\n",
      "train loss:0.012830361816875104\n",
      "train loss:0.028948279823705767\n",
      "train loss:0.027685213921136164\n",
      "train loss:0.04390343071302425\n",
      "train loss:0.02180451592777377\n",
      "train loss:0.027408790125422914\n",
      "train loss:0.036504824507677516\n",
      "train loss:0.024325172971666004\n",
      "train loss:0.011213436019675942\n",
      "train loss:0.03153445902757539\n",
      "train loss:0.031294763300392076\n",
      "train loss:0.01059239420446483\n",
      "train loss:0.05360893036719792\n",
      "train loss:0.018428529003184972\n",
      "train loss:0.04159242957458103\n",
      "=== epoch:20, train acc:0.988, test acc:0.956 ===\n",
      "train loss:0.026084534749027742\n",
      "train loss:0.02613330910460671\n",
      "train loss:0.019898763183919838\n",
      "train loss:0.022833731240501157\n",
      "train loss:0.03943664196032587\n",
      "train loss:0.02659751651493929\n",
      "train loss:0.030771306865632543\n",
      "train loss:0.02327848159810039\n",
      "train loss:0.013760185852218347\n",
      "train loss:0.03178198908209463\n",
      "train loss:0.012253576943057218\n",
      "train loss:0.04962927145449045\n",
      "train loss:0.012650243725239147\n",
      "train loss:0.037626921927671586\n",
      "train loss:0.0536931839723193\n",
      "train loss:0.02214320684129664\n",
      "train loss:0.016804342266057176\n",
      "train loss:0.029579678544885756\n",
      "train loss:0.037925472245487896\n",
      "train loss:0.015289876562692907\n",
      "train loss:0.01406392787327415\n",
      "train loss:0.034962889959579885\n",
      "train loss:0.027698413007508537\n",
      "train loss:0.014616860283588733\n",
      "train loss:0.01324643971368037\n",
      "train loss:0.03146395593562072\n",
      "train loss:0.030226851458620904\n",
      "train loss:0.06932920867064865\n",
      "train loss:0.010214937432921022\n",
      "train loss:0.01669799050053199\n",
      "train loss:0.015089253295734495\n",
      "train loss:0.019318872771401843\n",
      "train loss:0.02210972407800178\n",
      "train loss:0.013559852218367025\n",
      "train loss:0.010256291159682334\n",
      "train loss:0.028444694187937856\n",
      "train loss:0.01398861146305468\n",
      "train loss:0.04526436736373978\n",
      "train loss:0.011841921553383624\n",
      "train loss:0.00469710266202851\n",
      "train loss:0.019713144213051793\n",
      "train loss:0.01013554251977848\n",
      "train loss:0.019984169283667343\n",
      "train loss:0.010883875752491693\n",
      "train loss:0.015206511238236464\n",
      "train loss:0.019291935480398516\n",
      "train loss:0.005855910714030801\n",
      "train loss:0.04321585014271891\n",
      "train loss:0.014297235890788303\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.948\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxc5X3v8c9vRvtuLd4Nlm1ibBKKQQESQpodm6Ys2Uq2SyiJQ4E2uS1raQjkNhca0qQvbgmEJGRfoKxOcMBASNKSGPDCZhtjWza2JNtabMnWLo2e+8c5ssejGWm0nBlZ832/XvOas875zdHo+Z3znOc8x5xziIhI5gqlOwAREUkvJQIRkQynRCAikuGUCEREMpwSgYhIhlMiEBHJcIElAjO7z8wazey1BPPNzO40s+1m9oqZnR5ULCIikliQZwQ/ApYPM38FcJL/WgncHWAsIiKSQGCJwDn3R+DAMItcCPzEedYCZWY2K6h4REQkvqw0bnsOsCdqvM6ftjd2QTNbiXfWQGFh4Rknn3xySgIUEZkq1q9f3+ycq4o3L52JwOJMi9vfhXPuXuBegJqaGrdu3bog4xKRKebRjfXc8eRWGlq7mF2Wz7XnLeaiZXMyZvsAZvZmonnpTAR1wLyo8blAQ5piEZEp6tGN9dz48Kt09UUAqG/t4saHXwVISWH86MZ6bnj4Fbr7Bo5s/7qHXmFzQxunnTCNzt4IXX0Runr76eodoLOvn+7eSNR0772zN8IVf7mQ5W+dOeExpjMRrAKuNrNfAWcBbc65IdVCInL8S+URcVdvhH2Hutnb1sW+tm5uWbXpSBI4skxfhFtWbaIoN4vyohwqCnMoL8yhKDcLs3iVFYl190XY19bN3rZu9h3qYm9bN/uPjHfzWn0bAzF1Hb39A9z73zuBnUM+LycrRH52mIKcMPnZYfL99+K8LLLDo4stWYElAjP7JfAeoNLM6oCvAtkAzrl7gNXA+cB2oBO4LKhYRDJdOqsmJuqI3DnH4Z7+o4VuWxf72nqOFL6D09u6+pL6vNauPj7/k2OrmXPCIcoLc5hWeDQ5lPvDpQXZtHb2Hdn2YEHf2jl0eyV5WcwqzWdmad6QJDDIgCe+/G7ys8Pk5YQoyMkiLytEVjj1t3fZ8dYNta4RiIxObEEMkJ8d5raPvC2QZNDW1cfO5g52Nrezs6mD7/13LV1+tUg0A/Jzwkl/bmTA0dM/9HMqi3KZVZrHzNI8ZpXmMaMkL2o8n09/by0Nbd1D1ptRksu9n63hQEcvLR29HOjo8d7beznYOTjNGz/c0x+1vRxmluYxsyT/yHZmRm1zZmkeBTlHj7HPuf131Ld2Ddn+nLJ8nrvhfUl///Eys/XOuZp489JZNSSSEVJ5ND4w4DjY2UtTew+Nh3poOtzDrb9OXDWSlx2mND+bsoJsSvO9V0FOeMTqkZ7+CLtbOqlt7qC2yS/0mzvY2dxBc3vvkeVCRsIjYgd8+qwTkv5uITMqi3KPFLYzS7xCPydr+CPo65afHDcR3rhiCX8xryypbff0RzjU1U9Jfha5WcknL4Bn3OfJy2sZMr3bVQC1o/qsoCgRiAQo2WqRgQFHxDkiA96rf8AxMPjuT+/tH6ClwyvcGw9777HDze099CcqeWO0dvVxxc/WD5meHTZK87Mpyc+mzE8OZQU55OeEqT/YRW1zO/UHu44p4CuLcllQWcgHlsygurKQ6spCFlQVMq+8gPd98w8Jj4hv+qulo9mdYzK4n8eTjHOzwlQVjy4BDMrrGZoEhpueDqoakilvvEfkh7v72H+om8ZDPXQc08IjQmdfhO6oVh3RrTy6eiNs2H2QvsjQ/zEDsrNCRwr7sQiZVwBXFecyvdh794bzooZzueTetexNUDVy3+feTltnH21dfbR2ee9tXX20dvZxqKuP1q7eI+OdvRFml+VRXVnkFfR+YT+/spCSvOyEcaa6amrScA4O1cO3T0m8zPLbIbc46lVy7Hh2IYQm5pqBqoYkYw13RH7habOPXvyL09pj8AJke1T9cCKDrTzyBlt7+MPxkgB41SKXnTOfrJARNiMcChEOcey7QTgcImxGVsjIChsVRblU+YV/eWEO4dDIrUiuH6Zq5JTZpUnsxfGZiCPycbnjJOhoHDq9cDpcu21itjEQgeZtsO8V77X3Fdj3KnQN17kC8MQNI3yweQkhp8h7f8/18NaPTkzM0VvRGYFMNc452nv6aTrcwye+++dj6qwHhUNe4Rp78TFkUFWcy8zSfGaVHL34N6vUO8ouzs0mPyfkF/hZXouP7FDCOvVJcaEwFQVh0DpaYN/LgEF2AWTnQ06h955d4L2ycuKve8swye6WttHH0tsJjZujCvxXYP9m6Pf/zuEcmL4UZp0KM0+F1dck/qzrdkLP4ZjXoTjT/OlnfA4WvX/0MaMzAkmzibpY2h8ZoLm916sPb+8+cjH0yIXR9sE68+4jN+8kEhlwXP6uambGFvZFuUOb73Ufgr0vQ8s2KKiAkjmQOwvyZkB4+HrjtF0oHIhAbwf0dcVPAuBNb97uVT2EssDC3nso7L2OGc8CC8Eo29iPiXPQsh12r4U9a2H3896+H0ko62iSGEwOOQXDr/Pg5cnHFemF5je8l/N/X7mlXoFfc5lX6M86FSrfAuGoqrLhEkFBufdKMyUCCVSyF0sHbwLaF1VNs2/w5VfTNLf3EO8EtjQ/+0gd+bITyo6pL//641vinhHMKcvnn89fMvTDejug/lWo3wANG71XokLIQlA0E0pmQ8ksL0GUzD76XjwruQuF/b3Q2z7MkWDUq88v3Pu6jhb0g9N6O6HPf0WGfue4/vOM5JYb/L5lJ0DFoqGvkjmJ67JHOiPp7/H28+61sOd579Xp75/8aTDvLDjtUzC3xivs+zqHft+40/x9NJyGjaP7/hULYckF/tH+26DsxJGTY+H0xN9/klDVkAQqUdVIQU6YM6vLjxT0I92UM7MkjxmlecwoOVpHPr0kj8qinGGb83XftiBuYdydW0HeNZth/2tHC/yGjdD0+tGjveLZMHvZ0VfVYuhuhUMN3kXAQw0xr3qvQE9WQaVXuEd6kljYrysePModPOI9pook37u4GFtt8vg/Jv7Yj3wfBvrBRbz3gYj3Oma839snfV1wcJd3tN6yw0tAg7LyoHwhVMZJEt+oTrz9eWd5+30wcZUvhBPO9qafcDZUnDT+i6UTXTV0nFLVkKRcc3sPL+w8EDcJAHT2Rmhu72HutHxq5k/zCvxhbsoZq2GPyG+b4xVyAIVVMPt072hv9jKYfRoUx+vTZZ53JJhI9yEvKRz2k8NjVyVedukFQ1uLDF4UjG09klM49mqZ4RLBqR8f22c6B4f3+Ulhm5cYWrbD/k3w+uNH92syn3PWF2GeX/gXxe0cUwKmRCATYl9bN8/vbOH5nQd4YecBtjd6R8ZG/C5l55Tl85u/P3d8G+3thPb9Ua9Gr3AaHG7fP/z653zp6NF+yZyJqf/OK/Fe0/2u0odLBB/+9vi3ly5mfnXYLKiO+TtG+uDgm36S2A5rbkr8OZ9/Ktg44biomkk3JQIZNeccdQe7eH7nAZ6vbeGFXQd4s6UTgOLcLGrmT+Ojp8/lrAXl1Da285XHNg1punjteYtH3tBAxKuK2L/Ja6XRtPVooX94P/QeHrqOhbx/8KLpCY7oo7z/5lF86+NYqgvCcLZXRVS5yBsfLhGkwvHSMiqNlAhkRK2dvdQ2d7B132Fe8Av/wb5bygqyefv8cj579omcvaCCJbNKjmnbfvr9Z/KxcCPEVuM/PR2WRf2DtjdB4yavGd7+Td5w4+tHm+RhMO1E78h95ttg0Qe9wr5oBhTP8N6LZnitekJRGxuufjgVJsPRqApCGYESQQZIpvlmd1+EXS0d7GzqoNbvM2Zncwe1Te0cjLqQW1mUy1nV5VyxoJwzq8t5y/RiQsPd1DRc08Un/tm7WNu4GTqajs4rrPLaYddcBjNO8YarTh65KeBkpEJ4ciRDGZYSwRQXr/nmdQ++wh/faKQoL9sv7DtoaOs6pmnmjJJcqisLWfG2WSzw+45ZWFXEiRUF8W+ecg66DvrVNvuSq6Nfd59Xl/6W82D6KTBjqfc+kRcMVQiln5LhpKdEMMU452jp6PWO6Js6eN9vzmFLuG1I1UzT5lLex/dYUFXI2+dPo7pyHtVVXv8x8ysLKcrNgoEBr7lkZwt01EJTM+yMvijbeOyF2oHk+oE/4p/rj63GCYIKIZERKREcpzp7+48czR+pxmnuYGdTO4e6jzbd25UXv510lbXxysc6sM7d0NEMnc2wtRk2tvgFf7P37iJx1jav+qZohldPP33J0fr66FfxDLhtbuIvEXQSEJGkKBEcB9p7+vnjG008t735SMG/79CxvUnOLs2juqqQC0+bw4KKPJbktbBw4E34beLPtQejHgqXP8270FpQCeULYO7bobDSGy+ogEJ/XvFM7z2sn47IVKH/5kmq6XAPT2/Zz5pN+3huewu9kQGKc7NYNKOIdy6q8LsALmJRQScn9u8i98Drfoub1+C1rVGtbYbxd3/2Cvv88uAKdtXRi0x6SgRBG0XPjzubO1izaR9rNu9nw+6DOAfzyvO59KzZrFiQzaklHWQ1v+61stnzGqzb7FXpRH/mjFPg7Zd7LW1mLIV735M4thnBPxREdfQik58SQdCGaT45sGc9u/a8yes7drKnbg8DHc2Uc5jr8js5obKLCjtEds9BbMMh2BC1bnaBVy+/eMXR5pUzTvGO7kVERkmJII1CP3gfC4AF/ngkOwsKKgkXVfl18ov9+nm/nr54pteeflp18h1xqWpGREagRJBGd838V5YuWsAZSxZRUjGLcG7xxPf3rqoZERmBEkEaXXXF36c7BBERJuapyBLXrqe+m+4QRERGpEQQgM7eflb98DbmP3cdvQlOurpzK1IclYhIfKoammDPbW/muQe+yXW9d/NG8dnMvuIhVr/RNiHP7BURCYISwQRp6+rjttVbCG/4IV/Pvo+Dc97LWy67H7JyuWhZkQp+EZm0VDU0AZ7avJ8PffsP5G74AV/Pvo/IovOY5icBEZHJTmcE49DS3sMtv97Mr19u4Lppv+fK7B/B4vMJf/xHSgIictxQIhgD5xyrXm7g1l9v5nB3Hz9esp6/3HkvnPxh+NgPISsn3SGKiCRNiWCU9rV1c9Mjr/LM6438xbwyvrfoeab/+d9hyV97SSCcne4QRURGRYkgSc45fvXiHv7v41voGxjgpvOXcHn4N4SeuhWWXggf/YGSgIgcl3SxOEmP+I98PGVOCU986d18IbSK0FNfgVMuVhIQkeOazgiStGH3QYrzsvjF588m9Ny34Zlb4a0fhYvv1UNaROS4phIsSbVNHSysKiL0P9+E3/0rvO3jcNE9SgIictwLtGrIzJab2VYz225mN8SZf4KZPWtmG83sFTM7P8h4xmNHUztf5EEvCZz6N3Dxd5UERGRKCCwRmFkYuAtYASwFPmlmsY/E+hfgAefcMuAS4DtBxTMe7T39LGv/b1Y03QenXgIX3a0Hr4vIlBHkGcGZwHbnXK1zrhf4FXBhzDIOKPGHS4GGAOMZs9qmds4KbaE/XAAXfUdJQESmlCATwRxgT9R4nT8t2i3AZ8ysDlgNxO2g38xWmtk6M1vX1NQURKzDqm3qYKE10F++SElARKacIBNBvEdtuZjxTwI/cs7NBc4HfmpmQ2Jyzt3rnKtxztVUVVUFEOrwdjS1syjUQPaMxSnftohI0IJMBHXAvKjxuQyt+rkceADAOfdnIA+YdE9gr9/fzGxrITxdiUBEpp4gE8GLwElmVm1mOXgXg1fFLLMbeD+AmS3BSwSpr/sZQe/+rd5A5VvSG4iISAACSwTOuX7gauBJYAte66BNZvY1M7vAX+yfgC+Y2cvAL4HPOediq4/SKjLgyGvb4Y0oEYjIFBRoQ3jn3Gq8i8DR026OGt4MnBNkDOPV0NrFCa6OAUKEyhekOxwRkQmnvoZGsL2pnYXWQG/JCXrGgIhMSUoEI/Caju4lVKULxSIyNSkRjGBnYysLQnvVdFREpix1ljOCw/tqyaEfdEYgIlOUzghGEGrZ5g2oxZCITFFKBMM41N1HVfcub6TypLTGIiISFCWCYQxeKO7JrYT8aekOR0QkEEoEw6htamdhqIGBCp0NiMjUpUQwjB2Nh1lk9eTM1IViEZm61GpoGE376imzDrUYEpEpTWcEw4g0+Z3NVanFkIhMXUoECUQGHAVttd6Imo6KyBSmRJBA3cFO5lNPfzgfSuamOxwRkcAoESSww+9srqd0AYS0m0Rk6lIJl8Dgc4rVx5CITHVqNZTA7v3NzAk1E5pxcrpDEREJlM4IEuja9wYhnFoMiciUp0SQQPYBdTYnIplBiSCOts4+pvfuxmFQvjDd4YiIBEqJII4dze0ssga6CudCdl66wxERCZQSQRyDLYacqoVEJAMoEcRR29hGte0lb9aSdIciIhI4NR+No23vDvKsTy2GRCQj6IwgDtfktxhSr6MikgGUCGL0RwYoaldncyKSOZQIYuw52MV8V09PzjQoKE93OCIigVMiiFHb1M6iUAO90/R4ShHJDEoEMQZ7Hc1RZ3MikiHUaijG3oZ6KuwwzFRncyKSGXRGEKO/8XVvQBeKRSRDKBHEyD64wxvQPQQikiGUCKIc7OhlZt8e+kO5UDov3eGIiKSEEkGU2mbvQnFXyXwIhdMdjohISigRRNnR1MEiqyekO4pFJIMEmgjMbLmZbTWz7WZ2Q4JlPmFmm81sk5n9Ish4RrJrfwvzrIl8dTYnIhkksOajZhYG7gI+CNQBL5rZKufc5qhlTgJuBM5xzh00s+lBxZOMzr1vEDI9nlJEMkuQZwRnAtudc7XOuV7gV8CFMct8AbjLOXcQwDnXGGA8I7LmN7wBVQ2JSAYJMhHMAfZEjdf506K9BXiLmT1nZmvNbHm8DzKzlWa2zszWNTU1BRJsX2SA0vadejyliGScIBOBxZnmYsazgJOA9wCfBL5vZmVDVnLuXudcjXOupqqqasIDBdh9oJNqa6AzfzbkFASyDRGRySipRGBmD5nZX5nZaBJHHRDdGH8u0BBnmcecc33OuZ3AVrzEkHK1fouhSPmidGxeRCRtki3Y7wY+BWwzs9vNLJmOeF4ETjKzajPLAS4BVsUs8yjwXgAzq8SrKqpNMqYJtaPxEAtsL7lqMSQiGSapROCce9o592ngdGAX8JSZ/cnMLjOz7ATr9ANXA08CW4AHnHObzOxrZnaBv9iTQIuZbQaeBa51zrWM7yuNTUt9LfnWS646mxORDJN081EzqwA+A3wW2Aj8HHgXcCleHf8QzrnVwOqYaTdHDTvgH/1XWg00+S2G1NmciGSYpBKBmT0MnAz8FPhr59xef9b9ZrYuqOBSKa9tuzegpqMikmGSPSP4T+fc7+LNcM7VTGA8aXGgo5fZfXvozislr6Ai3eGIiKRUsheLl0Q36zSzaWZ2ZUAxpdyOpnYWhhroKVsIFq/Vq4jI1JVsIviCc651cMS/E/gLwYSUerVN7Sy0esLTdaFYRDJPslVDITMz/+LuYD9COcGFlVr1exuoskMMzFbTURHJPMkmgieBB8zsHry7g68AnggsqhTr3rsVgJA6mxORDJRsIrge+CLwd3hdR6wBvh9UUKkWPrDNG6hMy03NIiJplVQicM4N4N1dfHew4aReb/8A0zp30p+VTda0+ekOR0Qk5ZK9j+Ak4DZgKZA3ON05tyCguFJm94EOqmmgo2g+pXo8pYhkoGRbDf0Q72ygH69voJ/g3Vx23NvR1MFCa8CpWkhEMlSyiSDfOfcMYM65N51ztwDvCy6s1Nm5/wAn2n49nlJEMlayF4u7/S6ot5nZ1UA9kNbHSk6UQ3VvEDZHeKYSgYhkpmTPCL4MFAD/AJyB1/ncpUEFlUqu2Ws6qhZDIpKpRjwj8G8e+4Rz7lqgHbgs8KhSxDlHwSH/8QdKBCKSoUY8I3DORYAzzKZeJzwtHb3MjeyhPW8W5BSmOxwRkbRI9hrBRuAxM/svoGNwonPu4UCiSpEdje0stAZ6y/SwehHJXMkmgnKghWNbCjnguE4EtU3tXGANDMyYEg2gRETGJNk7i6fMdYFoTfW1FFoPA3NOSXcoIiJpk+ydxT/EOwM4hnPubyc8ohTq3a/O5kREkq0a+k3UcB5wMdAw8eGkVvbBwc7mlAhEJHMlWzX0UPS4mf0SeDqQiFKkpz9CedebdOcUk1c0Je6NExEZk2RvKIt1EnDCRAaSam+2dLKQejpLFujxlCKS0ZK9RnCYY68R7MN7RsFxq7apnWWhBqzqg+kORUQkrZKtGioOOpBU27N3H8utld45S9MdiohIWiVVNWRmF5tZadR4mZldFFxYweuo3wJAzgw9sF5EMluy1wi+6pxrGxxxzrUCXw0mpBRpUYshERFIPhHEWy7ZpqeTjnOOokO19FsW6PGUIpLhkk0E68zsW2a20MwWmNm3gfVBBhakpvYeThjYw+GCEyB83OYzEZEJkWwi+HugF7gfeADoAq4KKqig7Wj0Hk8ZKVfX0yIiybYa6gBuCDiWlNnZeJAaa6R7pi4Ui4gk22roKTMrixqfZmZPBhdWsA7WvUG2RShU01ERkaSrhir9lkIAOOcOchw/szjS+DqgzuZERCD5RDBgZke6lDCz+cTpjfR4kdu6wxvQ4ylFRJJuAnoT8D9m9gd//N3AymBCClZ3X4TK7jc5nD+d4twpd8O0iMioJXVG4Jx7AqgBtuK1HPonvJZDx51dLR0stHq6S/V4ShERSP5i8eeBZ/ASwD8BPwVuSWK95Wa21cy2m1nCVkdm9jEzc2ZWk1zYY1fb2M5C20to+uKgNyUiclxI9hrBl4C3A286594LLAOahlvBzMLAXcAKYCnwSTMb0kzHzIqBfwCeH0XcY7avbhfF1kXxnCWp2JyIyKSXbCLods51A5hZrnPudWCkQ+ozge3OuVrnXC/wK+DCOMv9H+AbQHeSsYxL1z6vxVDOTCUCERFIPhHU+fcRPAo8ZWaPMfKjKucAe6I/w592hJktA+Y556IfhTmEma00s3Vmtq6padgTkRGFW97wBtTZnIgIkPydxRf7g7eY2bNAKfDECKvFe+zXkSanZhYCvg18Lont3wvcC1BTUzPmZqvOOYrba+kJF5BbPHOsHyMiMqWMusc159wfRl4K8M4A5kWNz+XYs4hi4K3A7817VORMYJWZXeCcWzfauJLReLiHEwbqOVy2gFw9nlJEBBj7M4uT8SJwkplVm1kOcAmwanCmc67NOVfpnJvvnJsPrAUCSwIAOxrbWRRqYKBCN5KJiAwKLBE45/qBq4EngS3AA865TWb2NTO7IKjtDufNfY3MsgPkz9KFYhGRQYF2xu+cWw2sjpl2c4Jl3xNkLACH92wGoGiuOpsTERkUZNXQpDPQ7LUYskrdTCYiMiijEkF+63YihKG8Ot2hiIhMGhmRCB7dWM87b3uG6b272c0MHn2lMd0hiYhMGlP+gb2PbqznxodfpasvwsKcBt6IzObGh18F4KJlc0ZYW0Rk6pvyZwR3PLmVrr4IYSLMt33scLPp6otwx5Nb0x2aiMikMPXPCLo+R1Ve25HxK7NWcWXWKpq6SoHd6QtMRGSSmPJnBFXWNqrpIiKZZsonAhERGZ4SgYhIhlMiEBHJcEoEIiIZbuongsLpo5suIpJhpnzzUa7dlu4IREQmtal/RiAiIsNSIhARyXBKBCIiGU6JQEQkwykRiIhkOCUCEZEMp0QgIpLhlAhERDKcEoGISIZTIhARyXBKBCIiGU6JQEQkwykRiIhkOCUCEZEMp0QgIpLhlAhERDKcEoGISIZTIhARyXBKBCIiGU6JQEQkwykRiIhkuEATgZktN7OtZrbdzG6IM/8fzWyzmb1iZs+Y2YlBxiMiIkMFlgjMLAzcBawAlgKfNLOlMYttBGqcc6cCDwLfCCoeERGJL8gzgjOB7c65WudcL/Ar4MLoBZxzzzrnOv3RtcDcAOMREZE4gkwEc4A9UeN1/rRELgd+G2+Gma00s3Vmtq6pqWkCQxQRkSATgcWZ5uIuaPYZoAa4I95859y9zrka51xNVVXVBIYoIiJZAX52HTAvanwu0BC7kJl9ALgJ+EvnXE+A8YiISBxBnhG8CJxkZtVmlgNcAqyKXsDMlgHfBS5wzjUGGIuIiCQQWCJwzvUDVwNPAluAB5xzm8zsa2Z2gb/YHUAR8F9m9pKZrUrwcSIiEpAgq4Zwzq0GVsdMuzlq+ANBbl9EREYWaCIQEZks+vr6qKuro7u7O92hBCovL4+5c+eSnZ2d9DpKBCKSEerq6iguLmb+/PmYxWvUePxzztHS0kJdXR3V1dVJr6e+hkQkI3R3d1NRUTFlkwCAmVFRUTHqsx4lAhHJGFM5CQway3dUIhARyXBKBCIicTy6sZ5zbv8d1Tc8zjm3/45HN9aP6/NaW1v5zne+M+r1zj//fFpbW8e17ZEoEYiIxHh0Yz03Pvwq9a1dOKC+tYsbH351XMkgUSKIRCLDrrd69WrKysrGvN1kqNWQiGScW3+9ic0NhxLO37i7ld7IwDHTuvoiXPfgK/zyhd1x11k6u4Sv/vUpCT/zhhtuYMeOHZx22mlkZ2dTVFTErFmzeOmll9i8eTMXXXQRe/bsobu7my996UusXLkSgPnz57Nu3Tra29tZsWIF73rXu/jTn/7EnDlzeOyxx8jPzx/DHjiWzghERGLEJoGRpifj9ttvZ+HChbz00kvccccdvPDCC3z9619n8+bNANx3332sX7+edevWceedd9LS0jLkM7Zt28ZVV13Fpk2bKCsr46GHHhpzPNF0RiAiGWe4I3eAc27/HfWtXUOmzynL5/4vvmNCYjjzzDOPaet/55138sgjjwCwZ88etm3bRkVFxTHrVFdXc9pppwFwxhlnsGvXrgmJRWcEIiIxrj1vMfnZ4WOm5WeHufa8xRO2jcLCwiPDv//973n66af585//zMsvv8yyZcvi3guQm5t7ZDgcDtPf3z8hseiMQEQkxkXLvGdo3fHkVhpau5hdls+15y0+Mn0siouLOXz4cNx5bW1tTCKFZI0AAApeSURBVJs2jYKCAl5//XXWrl075u2MhRKBiEgcFy2bM66CP1ZFRQXnnHMOb33rW8nPz2fGjBlH5i1fvpx77rmHU089lcWLF3P22WdP2HaTYc7FfWjYpFVTU+PWrVuX7jBE5DizZcsWlixZku4wUiLedzWz9c65mnjL6xqBiEiGUyIQEclwSgQiIhlOiUBEJMMpEYiIZDglAhGRDKf7CEREYt1xEnQ0Dp1eOB2u3Tamj2xtbeUXv/gFV1555ajX/Y//+A9WrlxJQUHBmLY9Ep0RiIjEipcEhpuehLE+jwC8RNDZ2TnmbY9EZwQiknl+ewPse3Vs6/7wr+JPn/k2WHF7wtWiu6H+4Ac/yPTp03nggQfo6enh4osv5tZbb6Wjo4NPfOIT1NXVEYlE+MpXvsL+/ftpaGjgve99L5WVlTz77LNji3sYSgQiIilw++2389prr/HSSy+xZs0aHnzwQV544QWcc1xwwQX88Y9/pKmpidmzZ/P4448DXh9EpaWlfOtb3+LZZ5+lsrIykNiUCEQk8wxz5A7ALaWJ5132+Lg3v2bNGtasWcOyZcsAaG9vZ9u2bZx77rlcc801XH/99Xz4wx/m3HPPHfe2kqFEICKSYs45brzxRr74xS8Ombd+/XpWr17NjTfeyIc+9CFuvvnmwOPRxWIRkViF00c3PQnR3VCfd9553HfffbS3twNQX19PY2MjDQ0NFBQU8JnPfIZrrrmGDRs2DFk3CDojEBGJNcYmosOJ7oZ6xYoVfOpTn+Id7/CedlZUVMTPfvYztm/fzrXXXksoFCI7O5u7774bgJUrV7JixQpmzZoVyMVidUMtIhlB3VCrG2oREUlAiUBEJMMpEYhIxjjeqsLHYizfUYlARDJCXl4eLS0tUzoZOOdoaWkhLy9vVOup1ZCIZIS5c+dSV1dHU1NTukMJVF5eHnPnzh3VOkoEIpIRsrOzqa6uTncYk1KgVUNmttzMtprZdjO7Ic78XDO735//vJnNDzIeEREZKrBEYGZh4C5gBbAU+KSZLY1Z7HLgoHNuEfBt4N+CikdEROIL8ozgTGC7c67WOdcL/Aq4MGaZC4Ef+8MPAu83MwswJhERiRHkNYI5wJ6o8TrgrETLOOf6zawNqACaoxcys5XASn+03cy2jjGmytjPnmQU3/govvGb7DEqvrE7MdGMIBNBvCP72HZbySyDc+5e4N5xB2S2LtEt1pOB4hsfxTd+kz1GxReMIKuG6oB5UeNzgYZEy5hZFlAKHAgwJhERiRFkIngROMnMqs0sB7gEWBWzzCrgUn/4Y8Dv3FS+20NEZBIKrGrIr/O/GngSCAP3Oec2mdnXgHXOuVXAD4Cfmtl2vDOBS4KKxzfu6qWAKb7xUXzjN9ljVHwBOO66oRYRkYmlvoZERDKcEoGISIabkolgMndtYWbzzOxZM9tiZpvM7EtxlnmPmbWZ2Uv+K/inVx+7/V1m9qq/7SGPgzPPnf7+e8XMTk9hbIuj9stLZnbIzL4cs0zK95+Z3WdmjWb2WtS0cjN7ysy2+e/TEqx7qb/MNjO7NN4yAcR2h5m97v/9HjGzsgTrDvtbCDjGW8ysPurveH6CdYf9fw8wvvujYttlZi8lWDcl+3BcnHNT6oV3YXoHsADIAV4GlsYscyVwjz98CXB/CuObBZzuDxcDb8SJ7z3Ab9K4D3cBlcPMPx/4Ld59IGcDz6fxb70PODHd+w94N3A68FrUtG8AN/jDNwD/Fme9cqDWf5/mD09LQWwfArL84X+LF1syv4WAY7wFuCaJ38Cw/+9BxRcz/9+Bm9O5D8fzmopnBJO6awvn3F7n3AZ/+DCwBe8O6+PJhcBPnGctUGZms9IQx/uBHc65N9Ow7WM45/7I0Htgon9nPwYuirPqecBTzrkDzrmDwFPA8qBjc86tcc71+6Nr8e7zSZsE+y8Zyfy/j9tw8fllxyeAX070dlNlKiaCeF1bxBa0x3RtAQx2bZFSfpXUMuD5OLPfYWYvm9lvzeyUlAbm3d29xszW+917xEpmH6fCJST+50vn/hs0wzm3F7wDAGB6nGUmw778W7wzvHhG+i0E7Wq/+uq+BFVrk2H/nQvsd85tSzA/3ftwRFMxEUxY1xZBMrMi4CHgy865QzGzN+BVd/wF8P+AR1MZG3COc+50vJ5jrzKzd8fMnwz7Lwe4APivOLPTvf9GI6370sxuAvqBnydYZKTfQpDuBhYCpwF78apfYqX9twh8kuHPBtK5D5MyFRPBpO/awsyy8ZLAz51zD8fOd84dcs61+8OrgWwzq0xVfM65Bv+9EXgE7/Q7WjL7OGgrgA3Ouf2xM9K9/6LsH6wy898b4yyTtn3pX5j+MPBp51dmx0ritxAY59x+51zEOTcAfC/BttP6W/TLj48A9ydaJp37MFlTMRFM6q4t/PrEHwBbnHPfSrDMzMFrFmZ2Jt7fqSVF8RWaWfHgMN5FxddiFlsF/C+/9dDZQNtgFUgKJTwKS+f+ixH9O7sUeCzOMk8CHzKzaX7Vx4f8aYEys+XA9cAFzrnOBMsk81sIMsbo604XJ9h2Mv/vQfoA8Lpzri7ezHTvw6Sl+2p1EC+8Vi1v4LUmuMmf9jW8Hz1AHl6VwnbgBWBBCmN7F96p6yvAS/7rfOAK4Ap/mauBTXgtINYC70xhfAv87b7sxzC4/6LjM7yHDu0AXgVqUvz3LcAr2EujpqV1/+Elpb1AH95R6uV4152eAbb57+X+sjXA96PW/Vv/t7gduCxFsW3Hq1sf/A0OtqKbDawe7reQwv33U//39Qpe4T4rNkZ/fMj/eyri86f/aPB3F7VsWvbheF7qYkJEJMNNxaohEREZBSUCEZEMp0QgIpLhlAhERDKcEoGISIZTIhAJmN8b6m/SHYdIIkoEIiIZTolAxGdmnzGzF/x+479rZmEzazezfzezDWb2jJlV+cueZmZro/rzn+ZPX2RmT/sd3m0ws4X+xxeZ2YP+MwB+HnXn8+1mttn/nG+m6atLhlMiEAHMbAnwN3gdhJ0GRIBPA4V4fRqdDvwB+Kq/yk+A651zp+Ld/To4/efAXc7r8O6deHejgtfL7JeBpXh3m55jZuV4XSec4n/Ovwb7LUXiUyIQ8bwfOAN40X/S1PvxCuwBjnYo9jPgXWZWCpQ55/7gT/8x8G6/T5k5zrlHAJxz3e5oPz4vOOfqnNeB2kvAfOAQ0A1838w+AsTt80ckaEoEIh4DfuycO81/LXbO3RJnueH6ZBnu4UY9UcMRvKeD9eP1RPkQ3kNrnhhlzCITQolAxPMM8DEzmw5Hnjd8It7/yMf8ZT4F/I9zrg04aGbn+tM/C/zBec+VqDOzi/zPyDWzgkQb9J9JUeq8rrK/jNfvvkjKZaU7AJHJwDm32cz+Be9JUiG8XiavAjqAU8xsPd6T7P7GX+VS4B6/oK8FLvOnfxb4rpl9zf+Mjw+z2WLgMTPLwzub+N8T/LVEkqLeR0WGYWbtzrmidMchEiRVDYmIZDidEYiIZDidEYiIZDglAhGRDKdEICKS4ZQIREQynBKBiEiG+//Ov9HC8qhp4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('SourceCode')  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 处理花费时间较长的情况下减少数据 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 绘制图形\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAEgCAYAAADMo8jPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAcvklEQVR4nO3da3BV5dnG8XsDOe8kBEiAREAqSERRUCxSDVWnI2hFGg8gajmjohQoKlIL1Fq0KAUUFKW0SqUzWqlKLWhFHKqilQ5gFQ3lMEBADjEJOUISAqz3A+79pu9L+1xrpifz/H+flsy1bp6Vvfa+2JlZj5EgCAwAAB+1+E8vAACA/xRKEADgLUoQAOAtShAA4C1KEADgLUoQAOCtVmHCqampQevWrZ25pKQkeWZlZaWUy8nJkWcePnzYmampqbH6+vqImVlWVlaQm5vrPCcSichrqKiokHL19fXyzLS0NCm3b9++siAIstu2bRt06tRJyctrUIV59CYzM1PK7dmzpywIguwvzwnat2/vPCcjI0Neh7rm4uJieWZ2drYzc/DgQausrIyYnXqPKT+P6urqf+oazMwOHDggz+zRo4eU+/TTT8uCIMhOSUkJlNdCee/GtGzZUsopn1kxtbW1Uu7IkSPxezE9PT1o27at85wwnx/q63vixAl5prLG0tJSq66ujpiZqa9ZmM+w5ORkKdeihf79rGPHjlLuo48+ir9mTYUqwdatW9sdd9zhzJ155pnyzJUrV0q5iRMnyjNffPHFUH9vbm6uvfDCC85zEhIS5DX89re/lXI7duyQZ1588cVSbtKkScVmZp06dbK3337bmZ8yZYq8BlWYN8bgwYOl3MiRI+Pt0759e1u4cKHznEGDBsnrOHbsmJS7/fbb5ZnK+2XMmDHx48zMTBs9erTznDfffFNew5133inlfvSjH8kzX3nlFSl39tlnF5ud+sfIsGHDnHnlfRijltv1118vz3z//ffVXPxebNu2rf3whz90nhPmy8HatWulXFVVlTxzxIgRzsy0adPixxkZGXbLLbc4zykqKpLXcM4550g5tSzNzGbOnCnlotHoaf/1yq9DAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeogQBAN4K9bB8EATW2NjozI0aNUqeeeGFF0q5uro6eeZ5553nzDR92Li8vNx+/etfO8/p3r27vIabb75ZyoV5OHjz5s1y1uzUg7SrV6925tavXy/PVLN9+vSRZ65Zs0bOxiQkJEg7RUyYMEGe+Z3vfEf+u1Xr1q1zZpruDlJXV2effvqp85yRI0fKa3jqqaekXN++feWZTz75pJw1M4tGo3bZZZc5c8rPK2bo0KFSTt1Zxsxszpw5Uq6goCB+HI1G7dJLL3WeE2anoSFDhki5G2+8UZ75wAMPODOJiYnx40gkIu3cctddd8lrWLVqlZTr1auXPFPdSevv4ZsgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBbobZNy8vLs9mzZztz5eXl8syXXnpJyr311lvyzIMHDzoztbW18ePk5GTr0aOH85ww17VhwwYpp25pZWZWUlIi5ZYtW2ZmZhUVFbZixQpn/tlnn5XXsHTpUimnbmllZjZs2DApN2DAgPhxSUmJPf74485zGhoa5HW88sorUu4Xv/iFPLNr167OTFlZWfw4JSVF2vZvypQp8hratm0r5cJsL1ZVVSVnzU7diy+//LIzd9VVV8kz1feDmjMzW7t2rZyNUbe6Uz5jYpTtDs1ObWWp2rp1qzPT9B44efKktF2l+r4xM7vkkkuk3H333SfPrKmpkbOnwzdBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAt0LtGNPQ0GA7duxw5v4Vu6Bs2rRJnjlw4EBn5v33348fFxcX27hx45znhNlZ5cCBA1LuySeflGeuWrVKzpqd2gknPz/fmbvyyivlmUOGDJFyK1eulGcquxCdTiQScWbOOusseV6vXr2knLrrhZnZkiVLnJkvvvgifpyXl2ePPPKI85z+/fvLa1iwYIGUO3HihDwzIyNDzpqd2qHpgw8+cOZmzJghz7zzzjulXJhdVcaOHSvl1q1bFz8+fvy4HT582HlOmB2nzjzzTCmn7rhlpu2s0nQnrcbGRjt06JDznOTkZHkNyr1tZta+fXt55je+8Q05ezp8EwQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeCvUtmlVVVX2hz/8wZkLs8XZeeedJ+VOnjwpz+zevbszk5SUFD9OSUmRthcrKCiQ17Bx40YpN2DAAHlmaWmpnDUzS0tLs379+jlzDz30kDyzrq5Oyk2YMEGeOWfOHCk3c+bM+HFKSoqdf/75znOabm/l0qlTJynXt29feaZyLzbdirCyslLacq6wsFBew9y5c6XctGnT5JnZ2dly1uzU69WzZ09nbvny5fLMQYMGSblrrrlGntm7d285G5OYmGidO3d25sJsJ9n08+kfUbZ7jNm2bZsz09jYGD+urKy0V1991XnOFVdcIa/h/vvvl3JNt29zWbFihZw9Hb4JAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvBUJgkAPRyKlZlb8r1vOv1WXIAiyzZrddZl9eW3N9brMmt1r1lyvy4x78aumuV6XWZNraypUCQIA0Jzw61AAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLcoQQCAtyhBAIC3KEEAgLdahQlHo9GgTZs2zlxOTo48c8eOHVLujDPOkGeWlJQ4M7W1tVZfXx8xM0tMTAySk5Od52RmZsprqKyslHIJCQnyzJYtW0q5srKysiAIstXrat26tbyGlJQUdQ3yzIaGBil35MiRsiAIss3M2rVrF3Tp0sV5Tl1dnbyO3bt3S7lzzz1Xnrlt2zZnpr6+3hobGyNmZmlpaUFWVpbznKqqKnkNynvWLNxr9rWvfU3Kffrpp2VBEGS3atUqSEpKcubDvMeCIJByHTp0kGfW1tZKuZ07d8bvxYyMjCA7O9t5zq5du+R1KPe2Wbj7W1nj/v37raKiImJmlpWVFeTm5jrPUV8HM/1zMRKJyDPV16y6ujr+mjUVqgTbtGlj06ZNc+YmTpwozxw0aJCU+9nPfibPnD9/vjPz2muvxY+Tk5Otb9++znOuvfZaeQ1N5/8jYd6galktWbKk2Ey/rsLCQnkNvXr1knK//OUv5Zk7d+6Uch9++GFx7LhLly62YcMG5zlbtmyR1zFixAgpt3HjRnnmFVdcEWpeVlaW9P5ZvXq1vIbhw4dLuTCv2UsvvSTlunXrVmxmlpSUZPn5+c784MGD5TXU19dLuenTp8szP/jgAyn37W9/O34vZmdn209/+lPnOcOGDZPXMWvWLCn3l7/8RZ559913OzM33HBD/Dg3N9deeOEF5zknT56U17By5Uop16qVXk3vvfeelFuzZk3x6f6cX4cCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvBXqYfnq6mpbu3atMxdmFxT1Yc/FixfLM5WHUtevXx8/TkxMlHZoUHcUMTN78MEHpdzll18uz/zNb34j5ZYsWWJmp16HvLw8Z753797yGi644AIpF+a6Nm/eLOUuuuii+HFJSYm0KcKePXvkdYwePVrKff3rX5dnXnXVVc7MX//61/hxcnKy9FB5WlqavAblIWmzcDt/fPHFF3LW7NR19ezZ05lTNneI2bdvn5RTduCJef755+VsTHJysvXo0cOZe+qpp+SZ6i4/kydPlmeWlpY6MydOnPibY2WHlz/+8Y/yGtQdYxYuXCjPVDZwMTNbs2bNaf+cb4IAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG+F2jYtPT3dCgoKnLnOnTvLM9etWyflZsyYIc8cNGiQnDUzO3z4sL3wwgvO3Ny5c+WZhYWFUm7SpEnyzHHjxslZM7NIJGKJiYnOnLodm5nZypUrpVy7du3kmRUVFXI2JhKJWKtW7tt3+PDh8kx1e60XX3xRnnnhhReGmheNRqX3WJjtxR566CEpN336dHlm063eFKmpqX+z7d3f069fP3lmQ0ODlJszZ448s6qqSs7GlJWV2bJly5y5W265RZ65bds2KXfbbbfJM3/wgx84M42NjfHjuro6Kyoqcp5z5ZVXymt47733pNyHH34oz1S2GTQze/TRR0/753wTBAB4ixIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeCvUjjGHDh2yxx57zJm79tpr5Zmvv/66lDt8+LA884MPPnBmxowZEz9u3769jRw50nnO4sWL5TUMHjxYypWUlMgzv/e978lZs1PXNXnyZGdu79698sypU6dKuSeffFKeec8998jZmJYtW1pGRoYzt2vXLnlmx44dpdzBgwflmTfccIOcNTOrra21d99915n7/ve/L8/cvn27lLv33nvlmZ988omU+93vfmdmp+7FKVOmOPNvvfWWvIYePXpIOfW9aGbS++X/OnLkiLTDSZs2beSZ6o5Lq1atkmempqY6M03fU9XV1dLr8dlnn8lrUHdwmjZtmjwzTPZ0+CYIAPAWJQgA8BYlCADwFiUIAPAWJQgA8BYlCADwFiUIAPAWJQgA8BYlCADwFiUIAPBWqG3TOnfubI8++qgzp2z7FLN161YpF2Z7oFmzZjkz+/fvjx9Ho1G79NJLneekpaXJa1i4cKGUKysrk2eOHz9ezpqZ7d6920aMGOHMffzxx/LMRYsWSbn169fLM5cvXy7l+vTpEz8+fvy4lZeXO8/p2bOnvI65c+dKuZtuukmeWVlZ6cy89tpr8eOEhAQ744wznOc88cQT8hpmz54t5XJycuSZrVu3lrNmZnV1dbZlyxZnLiUlRZ6p3mNr166VZ+bl5cnZmPz8fPvTn/7kzI0dO1aeOXPmTCkXjUblmQsWLHBmmm5P2a5dOxs9erTznDDb0qmfH+PGjZNnrlixQs6eDt8EAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3ooEQaCHI5FSMyv+1y3n36pLEATZZs3uusy+vLbmel1mze41a67XZca9+FXTXK/LrMm1NRWqBAEAaE74dSgAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFutwoQjkUig5PLz8+WZR48elXKJiYnyzLKyMunvbWhoiJiZRaPRICsrS56vSE1NlXLHjx+XZ6alpUm5LVu2lAVBkJ2WliZdV319vbyGhIQEKVddXS3P7NChg5TbtWtXWRAE2WZm7dq1Czp37uw8Z+fOnfI6unTpIuUqKyvlmS1btnRmysvLraamJmJmlpKSEqSnpzvPqampkdeQk5Mj5U6ePCnP/Pzzz9VoWRAE2epnR+vWreU1tGnTRspVVFTIMzt27CjlioqK4vdiRkZGoPyM9+7dK6+jU6dOUu7AgQPyzMzMTGemqqrK6urqImZm6udHmPd6Xl6elGtsbJRnfvHFF1KupqYm/po1FaoEVc8++6yc3bJli5RTb04zs+eee86ZWbduXfw4KyvL7rnnHuc5J06ckNdw0UUXSbnS0lJ55sUXXyzlunbtWmx26rruvvtuZ37btm3yGnJzc6XcW2+9Jc+89957pdzNN99cHDvu3Lmzvffee85zCgsL5XUsXrxYyq1atUqeqXzw/PjHP44fp6en29ChQ53nvPPOO/IaJk6cKOWOHDkiz1RfsyAIit2p/3XFFVfI2VtvvVXKrVixQp45Y8YMKderV6/4deXk5Ni8efOc50yYMEFexyOPPCLlZs+eLc8cNGiQM7N8+fL4sfr5Eea9PmfOHCl38OBBeeYTTzwh5datW3fae5FfhwIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvBXqOcGcnBy7+eabnTn1oWMzs88++0zKvfrqq/LM3r17OzMbNmyIHx8/flx6wP6ss86S15CRkSHlwjx7uHLlSjlrZrZ//3574IEHnLnp06fLMwsKCqRcmGcPX3/9dTkb06JFC2nzAOVeiLnxxhulXJjnYJWHmZtuQJCQkCA93H711VfLa7jzzjul3NatW+WZw4cPl3Kx50pzc3Ol5+Suu+46eQ3nn3++lDt27Jg88+c//7mcjSktLbWnn37amQvzudj08+kfCfPeUZ4/ra2tjR9HIhFpkxJ1Aw0zs379+km5O+64Q545cOBAKdf02fCm+CYIAPAWJQgA8BYlCADwFiUIAPAWJQgA8BYlCADwFiUIAPAWJQgA8BYlCADwFiUIAPBWqG3T8vLybPbs2c7c/Pnz5ZnFxaf9P97/P/v27ZNnfvzxx85MZWVl/DglJcXOO+885zl//vOf5TW89NJLUi62rZQiKSlJzpqZXXTRRbZx40ZnbsGCBfLMd955R8pdcskl8sww90tMEATW0NDgzKWkpMgzH3zwQSlXWFgoz7zmmmucmaqqqvhxXV2dFRUVOc9RtmOLGTVqlJR75pln5JnqlmUxHTp0sGnTpjlz6hZvZiZtL2dmVl9fL88cPHiwlFu0aFH8ODk52c455xznOY8//ri8juuvv17K3XbbbfLMSCQiZ81Ofd50797dmQuzhV9JSYmUS09Pl2eOHTtWzp4O3wQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeCrVjzJ49e2zMmDHO3LvvvivPnD59upTLyMiQZyo7M/Tt2zd+fOzYMWlHmjZt2shreOONN6Tc5MmT5Zl33XWXlHv66afNzKy2ttbWr1/vzCu7XcQsXLhQyl133XXyzKVLl0q5prt5lJeX2/Lly53n9OnTR17H8OHDpVyYa4tGo85Mixb/+2/RkydP2tGjR53nXHXVVfIaZsyYIeU+/PBDeeYDDzwgZ83MtmzZYt26dXPmwuzuMm7cOCmnvAYxQ4cOlbMxKSkp1rNnT2cuzPvs0KFDUm7Tpk3yTGVnqIcffjh+XFVVZatXr3aeE2Y3r/z8fCl3++23yzM7dOgg5T755JPT/jnfBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3qIEAQDeogQBAN6iBAEA3gq1bVpycrK07c2cOXPkmT/5yU+kXJht00aOHOnM7Nmz52/++/jx485zZs6cKa9B3WLtu9/9rjxz/PjxctbMbP/+/dL2VjU1NfLMKVOmSLni4mJ5Zm5urpyNqa6utrVr1zpz5557rjxz//79Uk7d+snMrLS0VM6amWVmZtq1117rzJWXl8sz582bJ+XU7dXMzEaMGCHl1q1bZ2anPjuUbdMKCgrkNZx55plS7uqrr5ZnJicnS7mmnwUNDQ22a9cu5zmjR4+W1/HZZ59JudraWnnmfffd58w03bYuCALpc1G5X2PS09Ol3NSpU+WZb775ppSLRCKn/XO+CQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALwVCYJAD0cipWambwXy361LEATZZs3uusy+vLbmel1mze41a67XZca9+FXTXK/LrMm1NRWqBAEAaE74dSgAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW63ChDMzM4P27ds7c8ePH5dn1tTUSLl27drJM1u1cl/W/v377fDhwxEzs+Tk5CA9Pd15ThAE8hrUn0FCQoI8s0uXLlJu06ZNZUEQZCcmJgapqanOfJs2beQ1VFRUSLns7Gx5ZosW2r/Ftm3bVhYEQbaZ/pplZmbK66irq5Ny0WhUntnQ0ODMlJeXW01NTcTMLCkpKVDmh3mPKWswMztx4oQ884ILLpBysXsxOTlZuq7GxkZ5Dep68/Pz5ZkfffSRlDt58mT8XsRXW6gSbN++vS1atMiZUz8ozczefvttKTdu3Dh5pvKhfv3118eP09PTrbCw0HnOsWPH5DWoP4OOHTvKM5955hkpF4lEis3MUlNTraCgwJkfNmyYvIaXX35Zyk2YMEGeqRS1mVlBQUFx7Dg9Pf1vXsO/Z+DAgfI6ioqKpFz//v3lmbt27XJmHn744fhxNBqV1lxeXi6vYc+ePVKuurpanrlx40YpF7sXo9GoDR482JkvKSmR16Cud/369fJM9R9N1dXVxe4Uvgr4dSgAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuhnhPMyMiQnmHatGmTPPONN96QcmGeH5o1a5Yzs3fv3vhxamqq9enTx3nOXXfdJa/h1ltvlXKXXnqpPDPMs3dmZt26dbPf//73ztz9998vzxw6dKiU27lzpzzz+eefl7MxkUjEWrZs6cyFeZ5OfaZw8eLF8sznnnvOmVmyZEn8uLGx0crKypznhLlvLrvsMim3fft2eWaYZ4HNzDp16mTz58935h577DF55tGjR6Wc8gxwzEMPPSTlpkyZIs/Efze+CQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvEUJAgC8RQkCALxFCQIAvBVq27SioiI7//zznblHHnlEntnQ0CDlDh48KM/My8tzZhISEuLH9fX1tnXrVuc5a9askdewaNEiKads/RWzbds2OWtmtmfPHhszZowzN2/ePHnmqFGjpFyYLd7CvLYxrVu3tiFDhjhzYbbbe/HFF6XchRdeKM+MRCJy1swsCAJrbGx05vr37y/PXLFihZQLs9Z33nlHzpqZlZWV2bJly5y5qqoqeeahQ4dCrUGxe/fuf/pM/HfjmyAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBboXaMiUQilpKS4szt27dPnvnNb35Tym3fvl2emZOT48w03TGmtLTUli5d6jynpqZGXsPq1aul3Pjx4+WZYXdWqa2ttffff9+Zmz9/vjyzqKhIynXr1k2eqf79N954Y/z40KFD0k435557rryOs88+W8p961vfkmcuXLjQmZk7d278OAgCq6+vd54TZiecpvf6PzJp0iR5prJzVFOff/65TZ061Zm76aab5JkrV66UcmFmnjhxQs6ieeCbIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW5QgAMBblCAAwFuUIADAW6G2Tevatas9//zzzlx+fr4889FHH5VyhYWF8szXXnvNmQmCIH6clZVlAwcOdJ4zatQoeQ1du3aVcrfddps881e/+pWUi/2sotGo9e/f35k/evSovIakpCQpl52dLc9suh2aKiEhwdq1a+fMKfdCjLqF34EDB+SZys+r6VZdR48etc2bNzvPCXPfVFRUSLmm7wmXfv36SbkNGzaYmVlmZqZdfvnlznyvXr3kNQwYMEDKbdy4UZ4Z5meA5oFvggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9RggAAb1GCAABvUYIAAG9FwuyQEIlESs2s+F+3nH+rLkEQZJs1u+sy+/Lamut1mTW716y5XpeZB/civtpClSAAAM0Jvw4FAHiLEgQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4ixIEAHiLEgQAeIsSBAB4638AIjb/l4muFwkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('SourceCode')\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 随机进行初始化后的权重\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 学习后的权重\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
